\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\setlength\parindent{0pt}

\usepackage{listings}
%\usepackage{color}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}

\usepackage{verbatim}
\let\oldv\verbatim
\let\oldendv\endverbatim

%\userpackage{minted}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{light-gray}{gray}{0.95}


\lstset{frame=tb,
  language=Java,
  aboveskip=6mm,
  belowskip=6mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3,
  backgroundcolor=\color{light-gray},
  language=Matlab
}

%\usepackage{natbib} replaced by line below to make refernces work
\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[nottoc,numbib]{tocbibind} %to get references in table of contants
\usepackage{graphicx}

\usepackage{bm}

\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\usepackage{mdframed}
\usepackage{lipsum} % for creating dummy text
\mdfdefinestyle{MyFrame}{%
	linecolor=black,	
	backgroundcolor=gray!20!white,
	skipbelow = 8mm,
	skipabove = 8mm}

\usepackage{scrextend}

\usepackage{multimedia}
\usepackage{media9}

\usepackage{booktabs}
\usepackage{adjustbox}

\allowdisplaybreaks %Formulas multiple pages

\DeclareMathOperator\artanh{artanh}
\DeclareMathOperator\sech{sech}

\usepackage{amssymb}

\usepackage{mathtools}
\DeclarePairedDelimiter\set\{\}

\usepackage{lscape} %landscape 
\usepackage{pdflscape} %If screen friendly landscape (rotation)
\usepackage{diagbox} %For tables, headers
\usepackage{multirow} % For tables, splitting rows

%\usepackage{algorithm} % writing algos
%\usepackage[]{algorithm2e}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

%\usepackage[OLDMEK, 30]{masterfrontpage}
\usepackage{tabularx}
\usepackage{tabu}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} "\nameref*{#1}"}} % One single link

%\renewcommand{\subsectionmark}[1]{\markright{#1}{}}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\fancyhf{}
%\fancyhead[L]{\rightmark}
%\fancyhead[R]{\thepage}
%\renewcommand{\headrulewidth}{0pt}


\usepackage{fancyhdr,lipsum,setspace}
\pagestyle{fancy}           %define page style
\fancyhf{}                  % clear all header and footer fields
\renewcommand{\subsectionmark}[1]{%
	\ifsubsectioninheader
	\def\subsectiontitle{: #1}%
	\else
	\def\subsectiontitle{}%
	\fi}
\newif\ifsubsectioninheader
\def\subsectiontitle{}
\fancyhead[L]{\nouppercase{\rightmark\ifsubsectioninheader\subsectiontitle\fi}}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\addtolength{\headheight}{0.5pt} % make space for the rule
\fancypagestyle{plain}{%
	\fancyhead{} % get rid of headers on plain pages
	\fancyhead[R]{\thepage}
	\renewcommand{\headrulewidth}{0pt} % and the line
}
%\doublespacing
\renewcommand{\headrulewidth}{0.5pt}


\title{Derivation of the back propagation algorithm in the case of one hidden layer.}
\author{Karl Jacobsen}


\begin{document}

\maketitle
	
Notation: \\
$a_k^l$ is the activation at node $k$ in layer $l$. $a_k^L$ is the same in the output layer, and $a_k^{l-1}$ is the same in the input layer. \\
$z_k^l$ is the input to the activation function $f$ at node $k$ in layer $l$.\\
$w_{jk}^l$ is the weight from node $j$ in layer $l-1$ to node $k$ in layer $l$.\\

We are minimizing a cost function with respect to weights.
The weights will be updated with gradient descent

\begin{align}
	w_{ij}^l \leftarrow w_{ij}^l - \eta \frac{\partial{C}}{\partial{w_{ij}^l}}.
\end{align}

We start by calculating derivative of the cost function wrt. the output weights, $\frac{\partial C}{\partial w_{ij}^L}$:

\begin{align}
\frac{\partial C}{\partial w_{ij}^L} &= \frac{\partial}{\partial w_{ij}^L}\; \Big( \frac{1}{2} \sum_{k=1}^{N_L} (a_k^L - t_k)^2 \Big)\\
&=\frac{\partial}{\partial a_j^L}\; \Big( \frac{1}{2} \sum_{k=1}^{N_L} (a_k^L - t_k)^2 \Big)\;\frac{\partial a_j^L}{\partial w_{ij}^L}\\
&=(a_j^L - t_j) \; \frac{\partial (a_j^L - t_j)}{\partial a_j^L}\; \frac{\partial}{\partial w_{ij}^L}\; \Bigg(f\Big(z_j^L(w_{ij}^L) \Big)\Bigg)\\
&=(a_j^L - t_j) \;\frac{\partial}{\partial z_j^L} \;\Big(f(z_j^L) \Big)\; \frac{\partial z_j^L}{\partial w_{ij}^L}\\
&=(a_j^L - t_j) \; f'(z_j^L) \; \frac{\partial }{\partial w_{ij}^L}\;\Big(\sum_{m=1}^{N_L} (w_{mj}^L a_m^l + b_j^L) \Big)\\
&=(a_j^L - t_j) \; f'(z_j^L) \; a_i^l\\
&=\delta_j^L a_i^l.
\end{align}

$\frac{\partial C}{\partial w_{ij}^L}=\delta_j^L a_i^l$ can now be inserted into the gradient descent formula on the top, with $L$ instead of $l$, and we have a formula for the output weights. \\

Next we compute the derivative of the cost function wrt. the hidden layer weights

\begin{align*}
\frac{\partial C}{\partial w_{ij}^l} &= \frac{\partial}{\partial w_{ij}^l} \sum_{k=1}^{N_L} \frac{1}{2} \;(a_k^L - t_k)\\
&= \frac{\partial}{\partial a_j^l}\Big( \sum_{k=1}^{N_L}\; \frac{1}{2}\; (a_k^L - t_k)^2 \Big) \;\frac{\partial a_j^l}{\partial z_j^l} \; \frac{\partial z_j^l}{\partial w_{ij}^l}\\
&= \sum_{k=1}^{N_L} \;(a_k^L - t_k) \; \frac{\partial (a_k^L - t_k)}{\partial a_k^L} \; \frac{\partial a_k^L}{\partial a_j^l}\; \frac{\partial f(z_j^l)}{\partial z_j^l} \; \frac{\partial}{\partial w_{ij}^l} \Big(\sum_{m=1}^{N_L} w_{mj}^l a_m^{l-1} + b_j^l\Big)\\
&= \sum_{k=1}^{N_L} \;(a_k^L - t_k)\; \frac{\partial f(z_k^L)}{\partial z_k^L}\; \frac{\partial z_k^L}{\partial a_j^l}\; f'(z_j^l)\; a_i^{l-1}\\
&= \sum_{k=1}^{N_L} (a_k^L - t_k)\; f'(z_k^L)\; \frac{\partial (\sum_{m=1}^{N_L} w_{mk}^L a_m^l + b_k^L)}{\partial a_j^l}\; f'(z_j^l)\; a_i^{l-1}\\
&= \sum_{k=1}^{N_L} (a_k^L - t_k)\; f'(z_k^L)\;w_{jk}^L\; f'(z_j^l)\; a_i^{l-1}\\
&= \sum_{k=1}^{N_L} \delta_k^L \;w_{jk}^L\; f'(z_j^l)\; a_i^{l-1}\\
&= \delta_j^l\; a_i^{l-1}.
\end{align*}

$\frac{\partial C}{\partial w_{ij}^l} = \delta_j\; a_i^{l-1}$ can now be inserted into the gradient descent expression for the weights at the top.\\

Now we need to do the same for the biases. The derivations are exactly the same as for the weights, with the difference that the last chain in the chain rule now becomes $1$ and not $a_i$, since $\frac{\partial}{\partial b_j^l}\Big(\sum_{m=1}^{N_L} (w_{mj}^l a_m^{l-1} + b_j^l)\Big) = 1$. The same applies for for the output layer bias $b_j^L$. Full derivations can be found on p. 26 in my exercise notebook nr 1.\\ 

For the biases we end up with

\begin{align}
	\frac{\partial C}{\partial b_j^L} &= \delta_j^L
\end{align}

and 

\begin{align}
	\frac{\partial C}{\partial b_j^l} &= \delta_j^l
\end{align}

The biases are also updated by gradient descent, shown at the top of the page, but with $b$ instead of $w$. 

\end{document}
