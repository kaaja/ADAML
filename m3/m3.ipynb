{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit default\n",
    "__Note on how to use this notebook:__ <br>\n",
    "1) Save the notebook to disk. <br>\n",
    "2) Save the [data set](https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls) to the same folder that this notebook was saved in. \n",
    "\n",
    "[Data description](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#) <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas:\n",
    "Model-function Keras, layers in for loop, layer number as  input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following runs the data preperation that is used for all models.\n",
    "\n",
    "We scale all features by Sci-Kit learn's standard scaler. The standard scalars subtracts the mean, so that the means of the standardized variables equal zero. Furthermore the standard scaler divides the feautres by their respective variances, so that the variances of the standardized features equals one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty elements in data:  False\n",
      "Observations:  28497\n",
      "Percentage defaults:  21.31452433589501\n"
     ]
    }
   ],
   "source": [
    "# Reading file into data frame\n",
    "cwd = os.getcwd()\n",
    "filename = cwd + '/default of credit card clients.xls'\n",
    "nanDict = {}\n",
    "df = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "\n",
    "df.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "\n",
    "# Features and targets \n",
    "X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values\n",
    "y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values\n",
    "\n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:] \n",
    "\n",
    "# Train-test split\n",
    "trainingShare = 0.5 \n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare)\n",
    "\n",
    "# Input Scaling\n",
    "sc = StandardScaler()\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "XTest = sc.transform(XTest)\n",
    "\n",
    "# One-hot's of the target vector\n",
    "Y_train_onehot, Y_test_onehot = to_categorical(yTrain), to_categorical(yTest)\n",
    "\n",
    "# Remove instances with zeros only for past bill statements or paid amounts\n",
    "'''\n",
    "df = df.drop(df[(df.BILL_AMT1 == 0) &\n",
    "                (df.BILL_AMT2 == 0) &\n",
    "                (df.BILL_AMT3 == 0) &\n",
    "                (df.BILL_AMT4 == 0) &\n",
    "                (df.BILL_AMT5 == 0) &\n",
    "                (df.BILL_AMT6 == 0) &\n",
    "                (df.PAY_AMT1 == 0) &\n",
    "                (df.PAY_AMT2 == 0) &\n",
    "                (df.PAY_AMT3 == 0) &\n",
    "                (df.PAY_AMT4 == 0) &\n",
    "                (df.PAY_AMT5 == 0) &\n",
    "                (df.PAY_AMT6 == 0)].index)\n",
    "'''\n",
    "df = df.drop(df[(df.BILL_AMT1 == 0) &\n",
    "                (df.BILL_AMT2 == 0) &\n",
    "                (df.BILL_AMT3 == 0) &\n",
    "                (df.BILL_AMT4 == 0) &\n",
    "                (df.BILL_AMT5 == 0) &\n",
    "                (df.BILL_AMT6 == 0)].index)\n",
    "\n",
    "df = df.drop(df[(df.PAY_AMT1 == 0) &\n",
    "                (df.PAY_AMT2 == 0) &\n",
    "                (df.PAY_AMT3 == 0) &\n",
    "                (df.PAY_AMT4 == 0) &\n",
    "                (df.PAY_AMT5 == 0) &\n",
    "                (df.PAY_AMT6 == 0)].index)\n",
    "\n",
    "# Descriptive information\n",
    "print('Number of empty elements in data: ', df.isnull().values.any())\n",
    "print('Observations: ', df.shape[0])\n",
    "print('Percentage defaults: ', df['defaultPaymentNextMonth'].astype(bool).sum(axis=0)/df.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the same number of observations as in Yeh and Lien (2009). Yeh and Lien (2009) have 25 000 observations. However, we have the same number of observations as in Pyzhov and Pyzhov (2017), which is said to use the same dataset as Yeh and Lien (2009). \n",
    "\n",
    "The percentage of individuals with default is the same as in the Yeh and Lien (2009).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "We apply Sci-Kit learn's logistic regression method for performing classification of default and non-defaulting customers. It is possible to use regularization for the logistic regression. Regularization has the potential to reduce overfitting. We will apply Sci-Kit learn's Grid search function for identifying the optimal regularization value. The optimal regularization parameter is determined by the accuracy score on test sets applying K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': array([1.e+05, 1.e+04, 1.e+03, 1.e+02, 1.e+01, 1.e+00, 1.e-01, 1.e-02,\n",
       "       1.e-03, 1.e-04, 1.e-05, 1.e-06, 1.e-07])}],\n",
       "       pre_dispatch='2*n_jobs', refit='roc_auc', return_train_score='warn',\n",
       "       scoring=['accuracy', 'roc_auc'], verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lmbdas=np.logspace(-5,7,13)\n",
    "parameters = [{'C': 1./lmbdas}]\n",
    "scoring = ['accuracy', 'roc_auc']\n",
    "logReg = LogisticRegression()\n",
    "gridSearch = GridSearchCV(logReg, parameters, cv=5, scoring=scoring, refit='roc_auc') \n",
    "# \"refit\" gives the metric used deciding best model. \n",
    "# See more http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "gridSearch.fit(XTrain, yTrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchSummary(method, scoring):\n",
    "    method = eval(method)\n",
    "    if scoring == 'accuracy':\n",
    "        mean = 'mean_test_score'\n",
    "        sd = 'std_test_score'\n",
    "    elif scoring == 'auc':\n",
    "        mean = 'mean_test_roc_auc'\n",
    "        sd = 'std_test_roc_auc'\n",
    "    print(\"Best: %f using %s\" % (method.best_score_, method.best_params_))\n",
    "    means = method.cv_results_[mean]\n",
    "    stds = method.cv_results_[sd]\n",
    "    params = method.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.718870 using {'C': 10000.0}\n",
      "0.718870 (0.010002) with: {'C': 99999.99999999999}\n",
      "0.718870 (0.010002) with: {'C': 10000.0}\n",
      "0.718869 (0.010003) with: {'C': 1000.0}\n",
      "0.718870 (0.010001) with: {'C': 100.0}\n",
      "0.718862 (0.009999) with: {'C': 10.0}\n",
      "0.718818 (0.010011) with: {'C': 1.0}\n",
      "0.718451 (0.009942) with: {'C': 0.1}\n",
      "0.716281 (0.009712) with: {'C': 0.01}\n",
      "0.709592 (0.011814) with: {'C': 0.001}\n",
      "0.697470 (0.013815) with: {'C': 0.0001}\n",
      "0.691716 (0.015138) with: {'C': 1e-05}\n",
      "0.690935 (0.015475) with: {'C': 1e-06}\n",
      "0.690873 (0.015518) with: {'C': 1e-07}\n"
     ]
    }
   ],
   "source": [
    "gridSearchSummary('gridSearch', 'auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in terms of accuracy it does not matter much what the regularization parameter value is. The optimal parameter, among the chosen parameter values, is one, but the difference in test score between the eight first regularization parameter values is practically non-existent.\n",
    "\n",
    "### Logistic regression: Fitting and testing the model with the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=1,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = gridSearch.best_params_['C']\n",
    "logRegFinal = LogisticRegression(C=C, random_state=1)\n",
    "logRegFinal.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for printing accuracy results, confusion matrices and storing of these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfusionMatrix(method):\n",
    "    confusionArray = np.zeros(6, dtype=object)\n",
    "    method = eval(method)\n",
    "    \n",
    "    print('\\n###################  Training  ###############')\n",
    "    yPredTrain = method.predict(XTrain)\n",
    "    yPredTrain = (yPredTrain > 0.5)\n",
    "    cm = confusion_matrix(\n",
    "        yTrain, yPredTrain) \n",
    "    cm = np.around(cm/cm.sum(axis=1)[:,None], 2)\n",
    "    confusionArray[0] = cm\n",
    "    print('\\nTraining Confusion matrix: \\n', cm)\n",
    "    accScore = accuracy_score(yTrain, yPredTrain)\n",
    "    confusionArray[1] = accScore\n",
    "    print('\\nTraining Accuracy score: \\n', accScore)\n",
    "    AUC = roc_auc_score(yTrain, yPredTrain)\n",
    "    confusionArray[2] = AUC\n",
    "    print('\\nTrain AUC: \\n', AUC)\n",
    "    \n",
    "    print('\\n###################  Testing  ###############')\n",
    "    yPred = method.predict(XTest)\n",
    "    yPred = (yPred > 0.5)\n",
    "    cm = confusion_matrix(\n",
    "        yTest, yPred) \n",
    "    cm = np.around(cm/cm.sum(axis=1)[:,None], 2)\n",
    "    confusionArray[3] = cm\n",
    "    print('\\nTest Confusion matrix: \\n', cm)\n",
    "    accScore = accuracy_score(yTest, yPred)\n",
    "    confusionArray[4] = accScore\n",
    "    print('\\nTest Accuracy score: \\n', accScore)\n",
    "    AUC = roc_auc_score(yTest, yPred)\n",
    "    confusionArray[5] = AUC\n",
    "    print('\\nTestAUC: \\n', AUC)    \n",
    "    \n",
    "    return confusionArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[0.97 0.03]\n",
      " [0.77 0.23]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.8057333333333333\n",
      "\n",
      "Train AUC: \n",
      " 0.6020249101563981\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[0.97 0.03]\n",
      " [0.76 0.24]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8151333333333334\n",
      "\n",
      "TestAUC: \n",
      " 0.607885433715221\n"
     ]
    }
   ],
   "source": [
    "confusionArrayLogreg = createConfusionMatrix('gridSearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accurcies and AUC's on the testing set is higher the cooresponding best mean numbers on the validation sets from the K-fold cross validation. We see that only about $1/4$ of the defaults get correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN\n",
    "We will now perform classification by deep neural networks. Keras is used. \n",
    "\n",
    "\n",
    "### Grid search\n",
    "We will apply Sci-Kit learn's grid search function in order to determine the optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "def createModel(neurons =50, hiddenLayers = 2):\n",
    "    model = tf.keras.Sequential()\n",
    "    neuronsPerLayer = neurons // (hiddenLayers + 1)\n",
    "    model.add(tf.keras.layers.Dense(neuronsPerLayer, activation='relu', input_dim=XTrain.shape[1]))\n",
    "    for i in range(hiddenLayers):\n",
    "        model.add(tf.keras.layers.Dense(neuronsPerLayer, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(Y_train_onehot.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=createModel, verbose=0)\n",
    "\n",
    "neurons = [20, 50, 100, 200, 300, 400]# 500]\n",
    "hiddenLayers = [1, 2, 3, 5]\n",
    "batch_size = [5, 10, 32, 64]##, 40, 60, 80, 100]\n",
    "parameterGrid = [{'neurons': neurons, 'hiddenLayers': hiddenLayers, 'batch_size': batch_size}]\n",
    "folds = 3\n",
    "#scoring = ['accuracy', 'roc_auc']\n",
    "scoring = 'roc_auc'\n",
    "#grid = GridSearchCV(estimator=model, cv=folds, param_grid=parameterGrid, n_jobs=-1)\n",
    "#grid = GridSearchCV(estimator=model, cv=folds, param_grid=parameterGrid, n_jobs=-1, scoring=scoring, refit='roc_auc')\n",
    "grid = GridSearchCV(estimator=model, cv=folds, param_grid=parameterGrid, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "epochs = 10\n",
    "grid_result = grid.fit(XTrain, Y_train_onehot, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.761327 using {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 200}\n",
      "0.752635 (0.003034) with: {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 20}\n",
      "0.759858 (0.004175) with: {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 50}\n",
      "0.758405 (0.001298) with: {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 100}\n",
      "0.761327 (0.003652) with: {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 200}\n",
      "0.759914 (0.005431) with: {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 300}\n",
      "0.759342 (0.003141) with: {'batch_size': 5, 'hiddenLayers': 1, 'neurons': 400}\n",
      "0.738678 (0.018294) with: {'batch_size': 5, 'hiddenLayers': 2, 'neurons': 20}\n",
      "0.756553 (0.004414) with: {'batch_size': 5, 'hiddenLayers': 2, 'neurons': 50}\n",
      "0.760684 (0.004166) with: {'batch_size': 5, 'hiddenLayers': 2, 'neurons': 100}\n",
      "0.760575 (0.006323) with: {'batch_size': 5, 'hiddenLayers': 2, 'neurons': 200}\n",
      "0.758413 (0.006746) with: {'batch_size': 5, 'hiddenLayers': 2, 'neurons': 300}\n",
      "0.760983 (0.004713) with: {'batch_size': 5, 'hiddenLayers': 2, 'neurons': 400}\n",
      "0.661090 (0.114896) with: {'batch_size': 5, 'hiddenLayers': 3, 'neurons': 20}\n",
      "0.756789 (0.005422) with: {'batch_size': 5, 'hiddenLayers': 3, 'neurons': 50}\n",
      "0.751942 (0.006419) with: {'batch_size': 5, 'hiddenLayers': 3, 'neurons': 100}\n",
      "0.757454 (0.005788) with: {'batch_size': 5, 'hiddenLayers': 3, 'neurons': 200}\n",
      "0.756058 (0.010845) with: {'batch_size': 5, 'hiddenLayers': 3, 'neurons': 300}\n",
      "0.756919 (0.008571) with: {'batch_size': 5, 'hiddenLayers': 3, 'neurons': 400}\n",
      "0.729863 (0.011578) with: {'batch_size': 5, 'hiddenLayers': 5, 'neurons': 20}\n",
      "0.750266 (0.008779) with: {'batch_size': 5, 'hiddenLayers': 5, 'neurons': 50}\n",
      "0.756081 (0.005465) with: {'batch_size': 5, 'hiddenLayers': 5, 'neurons': 100}\n",
      "0.757827 (0.005635) with: {'batch_size': 5, 'hiddenLayers': 5, 'neurons': 200}\n",
      "0.759908 (0.006456) with: {'batch_size': 5, 'hiddenLayers': 5, 'neurons': 300}\n",
      "0.756716 (0.007425) with: {'batch_size': 5, 'hiddenLayers': 5, 'neurons': 400}\n",
      "0.740379 (0.007793) with: {'batch_size': 10, 'hiddenLayers': 1, 'neurons': 20}\n",
      "0.755101 (0.006280) with: {'batch_size': 10, 'hiddenLayers': 1, 'neurons': 50}\n",
      "0.758965 (0.001494) with: {'batch_size': 10, 'hiddenLayers': 1, 'neurons': 100}\n",
      "0.759868 (0.005765) with: {'batch_size': 10, 'hiddenLayers': 1, 'neurons': 200}\n",
      "0.760915 (0.007694) with: {'batch_size': 10, 'hiddenLayers': 1, 'neurons': 300}\n",
      "0.757811 (0.003849) with: {'batch_size': 10, 'hiddenLayers': 1, 'neurons': 400}\n",
      "0.737564 (0.003120) with: {'batch_size': 10, 'hiddenLayers': 2, 'neurons': 20}\n",
      "0.750367 (0.003484) with: {'batch_size': 10, 'hiddenLayers': 2, 'neurons': 50}\n",
      "0.758888 (0.007483) with: {'batch_size': 10, 'hiddenLayers': 2, 'neurons': 100}\n",
      "0.755355 (0.006827) with: {'batch_size': 10, 'hiddenLayers': 2, 'neurons': 200}\n",
      "0.758904 (0.006073) with: {'batch_size': 10, 'hiddenLayers': 2, 'neurons': 300}\n",
      "0.759580 (0.009552) with: {'batch_size': 10, 'hiddenLayers': 2, 'neurons': 400}\n",
      "0.741289 (0.001638) with: {'batch_size': 10, 'hiddenLayers': 3, 'neurons': 20}\n",
      "0.745528 (0.004434) with: {'batch_size': 10, 'hiddenLayers': 3, 'neurons': 50}\n",
      "0.756681 (0.002288) with: {'batch_size': 10, 'hiddenLayers': 3, 'neurons': 100}\n",
      "0.757506 (0.003752) with: {'batch_size': 10, 'hiddenLayers': 3, 'neurons': 200}\n",
      "0.759932 (0.008932) with: {'batch_size': 10, 'hiddenLayers': 3, 'neurons': 300}\n",
      "0.758146 (0.009395) with: {'batch_size': 10, 'hiddenLayers': 3, 'neurons': 400}\n",
      "0.643079 (0.101294) with: {'batch_size': 10, 'hiddenLayers': 5, 'neurons': 20}\n",
      "0.732920 (0.004481) with: {'batch_size': 10, 'hiddenLayers': 5, 'neurons': 50}\n",
      "0.748289 (0.006552) with: {'batch_size': 10, 'hiddenLayers': 5, 'neurons': 100}\n",
      "0.754186 (0.003358) with: {'batch_size': 10, 'hiddenLayers': 5, 'neurons': 200}\n",
      "0.755254 (0.006681) with: {'batch_size': 10, 'hiddenLayers': 5, 'neurons': 300}\n",
      "0.759503 (0.005297) with: {'batch_size': 10, 'hiddenLayers': 5, 'neurons': 400}\n",
      "0.720690 (0.010962) with: {'batch_size': 32, 'hiddenLayers': 1, 'neurons': 20}\n",
      "0.734880 (0.005324) with: {'batch_size': 32, 'hiddenLayers': 1, 'neurons': 50}\n",
      "0.738411 (0.004979) with: {'batch_size': 32, 'hiddenLayers': 1, 'neurons': 100}\n",
      "0.749444 (0.005535) with: {'batch_size': 32, 'hiddenLayers': 1, 'neurons': 200}\n",
      "0.754245 (0.003188) with: {'batch_size': 32, 'hiddenLayers': 1, 'neurons': 300}\n",
      "0.753874 (0.002703) with: {'batch_size': 32, 'hiddenLayers': 1, 'neurons': 400}\n",
      "0.709779 (0.014409) with: {'batch_size': 32, 'hiddenLayers': 2, 'neurons': 20}\n",
      "0.727602 (0.008439) with: {'batch_size': 32, 'hiddenLayers': 2, 'neurons': 50}\n",
      "0.745391 (0.005069) with: {'batch_size': 32, 'hiddenLayers': 2, 'neurons': 100}\n",
      "0.749215 (0.002479) with: {'batch_size': 32, 'hiddenLayers': 2, 'neurons': 200}\n",
      "0.754969 (0.002048) with: {'batch_size': 32, 'hiddenLayers': 2, 'neurons': 300}\n",
      "0.753366 (0.004244) with: {'batch_size': 32, 'hiddenLayers': 2, 'neurons': 400}\n",
      "0.646336 (0.066517) with: {'batch_size': 32, 'hiddenLayers': 3, 'neurons': 20}\n",
      "0.717587 (0.001614) with: {'batch_size': 32, 'hiddenLayers': 3, 'neurons': 50}\n",
      "0.742406 (0.008898) with: {'batch_size': 32, 'hiddenLayers': 3, 'neurons': 100}\n",
      "0.748806 (0.002182) with: {'batch_size': 32, 'hiddenLayers': 3, 'neurons': 200}\n",
      "0.748391 (0.007581) with: {'batch_size': 32, 'hiddenLayers': 3, 'neurons': 300}\n",
      "0.753157 (0.003626) with: {'batch_size': 32, 'hiddenLayers': 3, 'neurons': 400}\n",
      "0.612377 (0.085164) with: {'batch_size': 32, 'hiddenLayers': 5, 'neurons': 20}\n",
      "0.704500 (0.007554) with: {'batch_size': 32, 'hiddenLayers': 5, 'neurons': 50}\n",
      "0.732620 (0.012466) with: {'batch_size': 32, 'hiddenLayers': 5, 'neurons': 100}\n",
      "0.749401 (0.007221) with: {'batch_size': 32, 'hiddenLayers': 5, 'neurons': 200}\n",
      "0.747443 (0.004295) with: {'batch_size': 32, 'hiddenLayers': 5, 'neurons': 300}\n",
      "0.749793 (0.001115) with: {'batch_size': 32, 'hiddenLayers': 5, 'neurons': 400}\n",
      "0.698312 (0.015141) with: {'batch_size': 64, 'hiddenLayers': 1, 'neurons': 20}\n",
      "0.709951 (0.018517) with: {'batch_size': 64, 'hiddenLayers': 1, 'neurons': 50}\n",
      "0.731328 (0.006324) with: {'batch_size': 64, 'hiddenLayers': 1, 'neurons': 100}\n",
      "0.732579 (0.004603) with: {'batch_size': 64, 'hiddenLayers': 1, 'neurons': 200}\n",
      "0.742209 (0.005294) with: {'batch_size': 64, 'hiddenLayers': 1, 'neurons': 300}\n",
      "0.738951 (0.003840) with: {'batch_size': 64, 'hiddenLayers': 1, 'neurons': 400}\n",
      "0.681431 (0.022228) with: {'batch_size': 64, 'hiddenLayers': 2, 'neurons': 20}\n",
      "0.708015 (0.005565) with: {'batch_size': 64, 'hiddenLayers': 2, 'neurons': 50}\n",
      "0.722256 (0.004775) with: {'batch_size': 64, 'hiddenLayers': 2, 'neurons': 100}\n",
      "0.735922 (0.007543) with: {'batch_size': 64, 'hiddenLayers': 2, 'neurons': 200}\n",
      "0.740993 (0.008942) with: {'batch_size': 64, 'hiddenLayers': 2, 'neurons': 300}\n",
      "0.741449 (0.003569) with: {'batch_size': 64, 'hiddenLayers': 2, 'neurons': 400}\n",
      "0.667219 (0.035500) with: {'batch_size': 64, 'hiddenLayers': 3, 'neurons': 20}\n",
      "0.692399 (0.010771) with: {'batch_size': 64, 'hiddenLayers': 3, 'neurons': 50}\n",
      "0.725338 (0.007788) with: {'batch_size': 64, 'hiddenLayers': 3, 'neurons': 100}\n",
      "0.732521 (0.007576) with: {'batch_size': 64, 'hiddenLayers': 3, 'neurons': 200}\n",
      "0.740804 (0.005798) with: {'batch_size': 64, 'hiddenLayers': 3, 'neurons': 300}\n",
      "0.740791 (0.006799) with: {'batch_size': 64, 'hiddenLayers': 3, 'neurons': 400}\n",
      "0.625386 (0.037533) with: {'batch_size': 64, 'hiddenLayers': 5, 'neurons': 20}\n",
      "0.689124 (0.010693) with: {'batch_size': 64, 'hiddenLayers': 5, 'neurons': 50}\n",
      "0.688131 (0.022744) with: {'batch_size': 64, 'hiddenLayers': 5, 'neurons': 100}\n",
      "0.707230 (0.015664) with: {'batch_size': 64, 'hiddenLayers': 5, 'neurons': 200}\n",
      "0.737777 (0.009027) with: {'batch_size': 64, 'hiddenLayers': 5, 'neurons': 300}\n",
      "0.742681 (0.005911) with: {'batch_size': 64, 'hiddenLayers': 5, 'neurons': 400}\n"
     ]
    }
   ],
   "source": [
    "gridSearchSummary('grid_result', 'accuracy') # Note that it is AUC that is printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best combination of the chosen number of hidden layers and neuron numbers is one hidden layer and two hundred neurons. \n",
    "\n",
    "A batch size of 10 is the best among the chosen batch sizes.\n",
    "\n",
    "Next we apply the optimal combination of batch size and neuron number from the crossvalidation train a model on the full training set. The model based on the full training set will then be applied to measure the accuracy on predictions on the test set.  \n",
    "\n",
    "### FItting the best model: early stopping\n",
    "We see from the above that the validation accuracy declines for the highest number of neurons. The decline in validation accuracy is a sign of overfitting. In order to avoid overfitting we use a methods for \"early stopping\". Early stopping stops the simulations when the validation set performance has dropped a user given number of times in a row. In order for the model to be able to escape local minima, we allo the validation accuracy to drop a few times before breaking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs before early stopping:  6\n"
     ]
    }
   ],
   "source": [
    "hiddenLayers, neurons =  grid_result.best_params_['hiddenLayers'], grid_result.best_params_['neurons']\n",
    "batch_size = 5\n",
    "\n",
    "model = KerasClassifier(build_fn=createModel, verbose=0, neurons =neurons, hiddenLayers = hiddenLayers)\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                             min_delta=0,\n",
    "                                             patience=2, # argument represents the number of epochs before stopping once your loss starts to increase (stops improving)\n",
    "                                             verbose=0, \n",
    "                                             mode='auto')]#,\n",
    "                                             #restore_best_weights=True)] # Use best model\n",
    "history = model.fit(XTrain,\n",
    "                        Y_train_onehot,\n",
    "                        epochs=15, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=[XTest, Y_test_onehot],\n",
    "                        callbacks = callbacks)\n",
    "\n",
    "print('Number of epochs before early stopping: ', len(history.history['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices, accuracy scores and AUC-numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[0.95 0.05]\n",
      " [0.65 0.35]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.8198\n",
      "\n",
      "Train AUC: \n",
      " 0.6507076050747016\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[0.95 0.05]\n",
      " [0.64 0.36]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8184666666666667\n",
      "\n",
      "TestAUC: \n",
      " 0.6549347316881102\n"
     ]
    }
   ],
   "source": [
    "confusionArrayNN = createConfusionMatrix('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about a third of the defaulting customers are correctly predicted. However, the performance is considerablt better than for logistic regression, where only a fourth of the customers with default was correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok accuracy. Not impressing performance when it compes to predicting the defaults. Only a 3rd of the defaults in the test set is correctly predicted. <br>\n",
    "\n",
    "Yeh and Lien (2009) get: <br>\n",
    "Training accuracy: 0.81<br>\n",
    "Training AUC: 0.55 <br>\n",
    "Testing accuracy: 0.83<br>\n",
    "Testing AUC: 0.54 <br>\n",
    "\n",
    "We see that we get the accuracy the the same level as Yeh and Lien (2009), but the AUC is better. However, it is unclear wheteher it acutally is AUC Yeh and Lien (2009) applies, as they call it area ratio.<br>\n",
    "\n",
    "Pyzhov and Pyzhov (2017) get about the same acuracy as we do, but higher AUCs.\n",
    "\n",
    "# Principal component analyses\n",
    "We will explore the effects of training the network with principcal components. For many of the networks we do not expect the introduction of principcal components to improve the performance considerably. The difference between training and testing accuracy is small for many of the networks, indicating that there is little overfitting. Maybe for the network setuos where there are larger deviations between training and testing accuracy, the networks with mutiple hidden layers, there can be gains from introducing principcal components. \n",
    "\n",
    "We use the principal components that explain 95 per cent of total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84870179 0.04871288 0.02708606 0.01606241 0.0144179 ]\n",
      "0.9549810379752075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "'''\n",
    "pca = PCA(n_components = 4)\n",
    "pca.fit_transform(X)\n",
    "#pca.components_.T[:,0] # Displays component\n",
    "pca.explained_variance_ratio_\n",
    "'''\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "Xreduced = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 50)                300       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 10,602\n",
      "Trainable params: 10,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "15000/15000 [==============================] - 2s 135us/step - loss: 5.1919 - acc: 0.6777 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 2/100\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 3/100\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 4/100\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 5/100\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 6/100\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n"
     ]
    }
   ],
   "source": [
    "trainingShare = 0.5 \n",
    "XTrain, XTest, yTrain, yTest=train_test_split(Xreduced, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare)\n",
    "Y_train_onehot, Y_test_onehot = to_categorical(yTrain), to_categorical(yTest)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu', input_dim=XTrain.shape[1]))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(Y_train_onehot.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             min_delta = 0,\n",
    "                                             patience=5,\n",
    "                                             verbose=0,\n",
    "                                             mode='auto')]\n",
    "\n",
    "\n",
    "history = model.fit(XTrain,\n",
    "                    Y_train_onehot, \n",
    "                    epochs=100, \n",
    "                    batch_size=30,\n",
    "                    validation_data=[XTest, Y_test_onehot],\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both training and testing accuracy is reduced when using Principal components as predictors instead of all of the original features. However, we also observe that the difference between training and validation accuracy is smaller when using principal components as predictors instead of all the features from the original data set.\n",
    "\n",
    "## Kernel PCA\n",
    "We will try Sci-Kit learn's kernel PCA method. Kernel PCA, kPca, is a PCA-method that allows for non-linearity. <mark> More about this!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-64be72b0cefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkPCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#kPCA = TruncatedSVD(n_components=5, algorithm='arpack')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \"\"\"\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambdas_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[1;32m    237\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py\u001b[0m in \u001b[0;36m_get_kernel\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    164\u001b[0m         return pairwise_kernels(X, Y, metric=self.kernel,\n\u001b[1;32m    165\u001b[0m                                 \u001b[0mfilter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                 **params)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[0;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown kernel %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[0;34m(X, Y, gamma)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# exponentiate K in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA, TruncatedSVD\n",
    "kPCA = KernelPCA(n_components=2, kernel='rbf', gamma=10) #0.04\n",
    "#kPCA = TruncatedSVD(n_components=5, algorithm='arpack')\n",
    "X_reduced = kPCA.fit_transform(X)\n",
    "print(kPCA.explained_variance_ratio_)\n",
    "print(np.sum(kPCA.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kPCA did not work. It gave memory error.\n",
    "\n",
    "# Support Vector Machines (SVM)\n",
    "We will now apply the SVM classifier to make the classification. We start by running the standard SVM estimator, and then we try alternative methods that potentially increase accuracy in the presence of non-linearity in the data. By \"non-linearity\" we mean that the labels cannot be separated by a linear classification plane (line in 2D, 2D plane in 3D, hyperplane for higher dimensions than 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = [{'C':np.logspace(-3,3,7)}]\n",
    "\n",
    "svmNormal = LinearSVC(loss='hinge')\n",
    "\n",
    "folds = 5\n",
    "scoring = ['accuracy', 'roc_auc']\n",
    "\n",
    "gridSearchSVMNormal = GridSearchCV(svmNormal, cv = folds, param_grid=parameters, scoring=scoring, refit='roc_auc')\n",
    "SVMNormalCVResult = gridSearchSVMNormal.fit(XTrain, yTrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.709523 using {'C': 0.1}\n",
      "0.701141 (0.009772) with: {'C': 0.001}\n",
      "0.704422 (0.011538) with: {'C': 0.01}\n",
      "0.709523 (0.011666) with: {'C': 0.1}\n",
      "0.699474 (0.011011) with: {'C': 1.0}\n",
      "0.697959 (0.017775) with: {'C': 10.0}\n",
      "0.668904 (0.017263) with: {'C': 100.0}\n",
      "0.552346 (0.134670) with: {'C': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "gridSearchSummary('SVMNormalCVResult', 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[0.97 0.03]\n",
      " [0.76 0.24]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.8104666666666667\n",
      "\n",
      "Train AUC: \n",
      " 0.6041806409840026\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[0.97 0.03]\n",
      " [0.75 0.25]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8086\n",
      "\n",
      "TestAUC: \n",
      " 0.6084006145256893\n"
     ]
    }
   ],
   "source": [
    "confusionArraySVMNormal = createConfusionMatrix('SVMNormalCVResult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear SVM the accuracies look very similar to the accuracies from logistic regression and neural networks. About a fourth of the customers with problem loans is predicted correctly with standard SVM. \n",
    "\n",
    "SVM stands out from the other mentioned methods in that the testing accuracy and AUC is a little higher than the training accuracy, which is unusual.\n",
    "\n",
    "### Polynomial SVM\n",
    "Increasing the complexity by introducing polynomial variables may increase the quality of the separation. We will not perform an polynomial SVM-estimation where we use second degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=42, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=2)),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[0.92 0.08]\n",
      " [0.74 0.26]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.7736666666666666\n",
      "\n",
      "Train AUC: \n",
      " 0.5927078018812684\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[0.92 0.08]\n",
      " [0.75 0.25]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.7706666666666667\n",
      "\n",
      "Train AUC: \n",
      " 0.5836243908240721\n"
     ]
    }
   ],
   "source": [
    "confusionArraySVMPoly = createConfusionMatrix('polynomial_svm_clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resuls with the 2nd degree polynomial are worse than with the normal SVM. Only a fourth of the customers with defauls is correctly predicted. Higher degree polynomial might work better. \n",
    "### SVM: Polynomial kernel\n",
    "The computational cost quickly becomes large for higher degree polynomials. \n",
    "\n",
    "A remedy is to use the so-called Kernel trick to effectively apply higher degree polynomials. With the kernel-trick we get higher degree polynomials without the extra computational cost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svmPolynomialKernel = SVC(kernel='poly')\n",
    "parameters = [{'degree': np.array((2,3)), 'C': [1.0]},\n",
    "              {'C':np.logspace(-1,1,3), 'degree': [3]}]\n",
    "scoring = ['accuracy', 'roc_auc']\n",
    "folds = 5\n",
    "smvPolyKernelGridSearch = GridSearchCV(svmPolynomialKernel, cv = folds, param_grid=parameters, scoring=scoring,\n",
    "                                       refit='roc_auc')\n",
    "smvPolyKernelGridSearchResult = smvPolyKernelGridSearch.fit(XTrain, yTrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.695678 using {'C': 1.0, 'degree': 3}\n",
      "0.682577 (0.008191) with: {'C': 1.0, 'degree': 2}\n",
      "0.695678 (0.013066) with: {'C': 1.0, 'degree': 3}\n",
      "0.694016 (0.012264) with: {'C': 0.1, 'degree': 3}\n",
      "0.695678 (0.013066) with: {'C': 1.0, 'degree': 3}\n",
      "0.692909 (0.015770) with: {'C': 10.0, 'degree': 3}\n"
     ]
    }
   ],
   "source": [
    "gridSearchSummary('smvPolyKernelGridSearchResult', 'auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three degrees works better than ten degrees. The AUC is higher for three degrees than for ten degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split3_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split4_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 7.53603115,  8.95945077,  5.82700062,  8.97353044, 26.62665639]),\n",
       " 'std_fit_time': array([0.36869547, 0.75958885, 0.24694452, 0.7656885 , 2.03865925]),\n",
       " 'mean_score_time': array([0.95948038, 0.95194483, 0.96077495, 0.94495735, 0.9262002 ]),\n",
       " 'std_score_time': array([0.00898825, 0.00857229, 0.0025406 , 0.00466272, 0.00522582]),\n",
       " 'param_C': masked_array(data=[1.0, 1.0, 0.1, 1.0, 10.0],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_degree': masked_array(data=[2, 3, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1.0, 'degree': 2},\n",
       "  {'C': 1.0, 'degree': 3},\n",
       "  {'C': 0.1, 'degree': 3},\n",
       "  {'C': 1.0, 'degree': 3},\n",
       "  {'C': 10.0, 'degree': 3}],\n",
       " 'split0_test_accuracy': array([0.78266667, 0.8       , 0.78833333, 0.8       , 0.80266667]),\n",
       " 'split1_test_accuracy': array([0.78033333, 0.79766667, 0.79066667, 0.79766667, 0.798     ]),\n",
       " 'split2_test_accuracy': array([0.779     , 0.80066667, 0.78966667, 0.80066667, 0.808     ]),\n",
       " 'split3_test_accuracy': array([0.78133333, 0.80366667, 0.78833333, 0.80366667, 0.806     ]),\n",
       " 'split4_test_accuracy': array([0.78233333, 0.80366667, 0.78933333, 0.80366667, 0.80133333]),\n",
       " 'mean_test_accuracy': array([0.78113333, 0.80113333, 0.78926667, 0.80113333, 0.8032    ]),\n",
       " 'std_test_accuracy': array([0.0013433 , 0.00229589, 0.00087939, 0.00229589, 0.00351252]),\n",
       " 'rank_test_accuracy': array([5, 2, 4, 2, 1], dtype=int32),\n",
       " 'split0_train_accuracy': array([0.78391667, 0.81525   , 0.79425   , 0.81525   , 0.83383333]),\n",
       " 'split1_train_accuracy': array([0.78358333, 0.81825   , 0.7945    , 0.81825   , 0.83691667]),\n",
       " 'split2_train_accuracy': array([0.78625   , 0.81575   , 0.79633333, 0.81575   , 0.83233333]),\n",
       " 'split3_train_accuracy': array([0.78241667, 0.81525   , 0.7915    , 0.81525   , 0.83416667]),\n",
       " 'split4_train_accuracy': array([0.78291667, 0.81666667, 0.79525   , 0.81666667, 0.83675   ]),\n",
       " 'mean_train_accuracy': array([0.78381667, 0.81623333, 0.79436667, 0.81623333, 0.8348    ]),\n",
       " 'std_train_accuracy': array([0.0013233 , 0.00113333, 0.00160606, 0.00113333, 0.00177216]),\n",
       " 'split0_test_roc_auc': array([0.67461441, 0.71541606, 0.70940995, 0.71541606, 0.70089181]),\n",
       " 'split1_test_roc_auc': array([0.6834598 , 0.67904954, 0.67631397, 0.67904954, 0.67109221]),\n",
       " 'split2_test_roc_auc': array([0.67404229, 0.69503268, 0.69601216, 0.69503268, 0.71756207]),\n",
       " 'split3_test_roc_auc': array([0.69657039, 0.70403862, 0.70411228, 0.70403862, 0.69183127]),\n",
       " 'split4_test_roc_auc': array([0.68419732, 0.68485246, 0.68423156, 0.68485246, 0.68316744]),\n",
       " 'mean_test_roc_auc': array([0.68257684, 0.69567787, 0.69401598, 0.69567787, 0.69290896]),\n",
       " 'std_test_roc_auc': array([0.00819102, 0.01306603, 0.01226369, 0.01306603, 0.01577042]),\n",
       " 'rank_test_roc_auc': array([5, 1, 3, 1, 4], dtype=int32),\n",
       " 'split0_train_roc_auc': array([0.70831481, 0.74997569, 0.73046761, 0.74997569, 0.77689309]),\n",
       " 'split1_train_roc_auc': array([0.71687012, 0.75995726, 0.73511783, 0.75995726, 0.78275393]),\n",
       " 'split2_train_roc_auc': array([0.70181892, 0.74858141, 0.72976332, 0.74858141, 0.7749997 ]),\n",
       " 'split3_train_roc_auc': array([0.71257951, 0.7536967 , 0.73264384, 0.7536967 , 0.78114523]),\n",
       " 'split4_train_roc_auc': array([0.71172953, 0.75653925, 0.73320563, 0.75653925, 0.78350938]),\n",
       " 'mean_train_roc_auc': array([0.71026258, 0.75375006, 0.73223965, 0.75375006, 0.77986027]),\n",
       " 'std_train_roc_auc': array([0.00502532, 0.00417786, 0.00193146, 0.00417786, 0.00333975])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smvPolyKernelGridSearchResult.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[0.98 0.02]\n",
      " [0.76 0.24]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.8162\n",
      "\n",
      "Train AUC: \n",
      " 0.6110393369497519\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[0.97 0.03]\n",
      " [0.78 0.22]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8040666666666667\n",
      "\n",
      "TestAUC: \n",
      " 0.5960181699035462\n"
     ]
    }
   ],
   "source": [
    "confusionArraySvmKernel = createConfusionMatrix('smvPolyKernelGridSearchResult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a gain increasing the degree from two, which we used with standard polynomial SVM, to three, which was the best degree found with polynomial kernel. The result is close to the result for standard SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Gaussian RBF Kernel\n",
    "Another kernel method is the so-called Gaussian Radial Basis Function (RBF) method. Following Geron (2017) p. 153, the following transformation is used $$\\phi_\\gamma (\\hat{x}, l) = \\exp(-\\gamma ||\\hat{x} - l||^2), $$\n",
    "\n",
    "where $l$ is the position of so-called landmarks. One often applies landmarks for every instancein the data set. This increases the number of features from the original feature number to the number of instances. The new variables represents a higher dimensional space compared to the original feature space, and the chance that the new features are linearly separable is increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = [{'gamma': np.logspace(-1,2,4), 'C': np.logspace(-1,1,3)}]\n",
    "#parameters = [{'gamma': np.array((.1, 1)), 'C': np.array((.001, 1000))}]\n",
    "#parameters = [{'gamma': np.array((.1, 1))}]\n",
    "\n",
    "folds = 5\n",
    "svmKernel = SVC(kernel='rbf')\n",
    "scoring = ['accuracy', 'roc_auc']\n",
    "svmKernelGridSearch = GridSearchCV(svmKernel, cv = folds, param_grid=parameters, scoring=scoring, refit='roc_auc')\n",
    "svmKernelGridSearchResult = svmKernelGridSearch.fit(XTrain, yTrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svmKernelGridSearchResult.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.709665 using {'C': 0.1, 'gamma': 0.1}\n",
      "0.709665 (0.003858) with: {'C': 0.1, 'gamma': 0.1}\n",
      "0.674762 (0.005201) with: {'C': 0.1, 'gamma': 1.0}\n",
      "0.616270 (0.007552) with: {'C': 0.1, 'gamma': 10.0}\n",
      "0.555003 (0.008345) with: {'C': 0.1, 'gamma': 100.0}\n",
      "0.708395 (0.005685) with: {'C': 1.0, 'gamma': 0.1}\n",
      "0.672249 (0.005904) with: {'C': 1.0, 'gamma': 1.0}\n",
      "0.617239 (0.007812) with: {'C': 1.0, 'gamma': 10.0}\n",
      "0.556863 (0.008124) with: {'C': 1.0, 'gamma': 100.0}\n",
      "0.694284 (0.012556) with: {'C': 10.0, 'gamma': 0.1}\n",
      "0.648543 (0.008121) with: {'C': 10.0, 'gamma': 1.0}\n",
      "0.614746 (0.011279) with: {'C': 10.0, 'gamma': 10.0}\n",
      "0.558185 (0.007859) with: {'C': 10.0, 'gamma': 100.0}\n"
     ]
    }
   ],
   "source": [
    "gridSearchSummary('svmKernelGridSearchResult', 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[0.96 0.04]\n",
      " [0.7  0.3 ]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.8156666666666667\n",
      "\n",
      "Train AUC: \n",
      " 0.6316046322708714\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[0.96 0.04]\n",
      " [0.72 0.28]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8111333333333334\n",
      "\n",
      "TestAUC: \n",
      " 0.6218802874564073\n"
     ]
    }
   ],
   "source": [
    "confusionArraySvmKernel = createConfusionMatrix('svmKernelGridSearchResult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test AUC is higer than with standard SVM and polynomial SVM. For the Gaussian kernel estimator, 28 per cent of the defaulted customers are predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Yeh, I-C. and Lien, C-h. (2009). The comparisons of data mining techniques for the predictive\n",
    "accuracy of probability of default of credit card clients. <br>\n",
    "_Expert Systems with Applications_ 36 (2009) 24732480. <br>\n",
    "https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf\n",
    "\n",
    "Pyzhov, V. and Pyzhov, S. (2017). Comparison of methods of data mining\n",
    "techniques for the predictive accuracy. _MPRA Paper_ No. 79326. <br>\n",
    "https://mpra.ub.uni-muenchen.de/79326/1/MPRA_paper_79326.pdf\n",
    "\n",
    "Geron, A. (2017). Hands-on machine learning with Sci-Kit learn and Tensorflow. O'Reilly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
