{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit default\n",
    "__Note on how to use this notebook:__ <br>\n",
    "1) Save the notebook to disk. <br>\n",
    "2) Save the [data set](https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls) to the same folder that this notebook was saved in. \n",
    "\n",
    "[Data description](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#) <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas:\n",
    "Model-function Keras, layers in for loop, layer number as  input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following runs the data preperation that is used for all models.\n",
    "\n",
    "We scale all features by Sci-Kit learn's standard scaler. The standard scalars subtracts the mean, so that the means of the standardized variables equal zero. Furthermore the standard scaler divides the feautres by their respective variances, so that the variances of the standardized features equals one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty elements in data:  False\n",
      "Observations:  30000\n",
      "Percentage defaults:  22.12\n"
     ]
    }
   ],
   "source": [
    "# Reading file into data frame\n",
    "cwd = os.getcwd()\n",
    "filename = cwd + '/default of credit card clients.xls'\n",
    "nanDict = {}\n",
    "df = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "\n",
    "df.rename(index=str, columns={\"default payment next month\": \"defaultPaymentNextMonth\"}, inplace=True)\n",
    "\n",
    "# Features and targets \n",
    "X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values\n",
    "y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values\n",
    "\n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:] \n",
    "\n",
    "# Train-test split\n",
    "trainingShare = 0.5 \n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare)\n",
    "\n",
    "# Input Scaling\n",
    "sc = StandardScaler()\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "XTest = sc.transform(XTest)\n",
    "\n",
    "# One-hot's of the target vector\n",
    "Y_train_onehot, Y_test_onehot = to_categorical(yTrain), to_categorical(yTest)\n",
    "\n",
    "# Descriptive information\n",
    "print('Number of empty elements in data: ', df.isnull().values.any())\n",
    "print('Observations: ', df.shape[0])\n",
    "print('Percentage defaults: ', df['defaultPaymentNextMonth'].astype(bool).sum(axis=0)/df.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the same number of observations as in Yeh and Lien (2009). Yeh and Lien (2009) have 25 000 observations. However, we have the same number of observations as in Pyzhov and Pyzhov (2017), which is said to use the same dataset as Yeh and Lien (2009). \n",
    "\n",
    "The percentage of individuals with default is the same as in the Yeh and Lien (2009).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "We apply Sci-Kit learn's logistic regression method for performing classification of default and non-defaulting customers. It is possible to use regularization for the logistic regression. Regularization has the potential to reduce overfitting. We will apply Sci-Kit learn's Grid search function for identifying the optimal regularization value. The optimal regularization parameter is determined by the accuracy score on test sets applying K-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': array([1.e+05, 1.e+04, 1.e+03, 1.e+02, 1.e+01, 1.e+00, 1.e-01, 1.e-02,\n",
       "       1.e-03, 1.e-04, 1.e-05, 1.e-06, 1.e-07])}],\n",
       "       pre_dispatch='2*n_jobs', refit='accuracy',\n",
       "       return_train_score='warn', scoring=['accuracy', 'roc_auc'],\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lmbdas=np.logspace(-5,7,13)\n",
    "parameters = [{'C': 1./lmbdas}]\n",
    "scoring = ['accuracy', 'roc_auc']\n",
    "logReg = LogisticRegression()\n",
    "gridSearch = GridSearchCV(logReg, parameters, cv=5, scoring=scoring, refit='accuracy') \n",
    "# \"refit\" gives the metric used deciding best model. \n",
    "# See more http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "gridSearch.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter:  {'C': 0.1} \n",
      "\n",
      "Parameter  {'C': 99999.99999999999} Train accuracy 0.811 Test accuracy 0.810 Train AUC 0.724 Test AUC 0.722  {'C': 99999.99999999999}\n",
      "Parameter  {'C': 10000.0} Train accuracy 0.811 Test accuracy 0.810 Train AUC 0.724 Test AUC 0.722  {'C': 10000.0}\n",
      "Parameter  {'C': 1000.0} Train accuracy 0.811 Test accuracy 0.810 Train AUC 0.724 Test AUC 0.722  {'C': 1000.0}\n",
      "Parameter  {'C': 100.0} Train accuracy 0.811 Test accuracy 0.811 Train AUC 0.724 Test AUC 0.722  {'C': 100.0}\n",
      "Parameter  {'C': 10.0} Train accuracy 0.811 Test accuracy 0.810 Train AUC 0.724 Test AUC 0.722  {'C': 10.0}\n",
      "Parameter  {'C': 1.0} Train accuracy 0.811 Test accuracy 0.810 Train AUC 0.724 Test AUC 0.722  {'C': 1.0}\n",
      "Parameter  {'C': 0.1} Train accuracy 0.811 Test accuracy 0.811 Train AUC 0.724 Test AUC 0.721  {'C': 0.1}\n",
      "Parameter  {'C': 0.01} Train accuracy 0.810 Test accuracy 0.809 Train AUC 0.723 Test AUC 0.721  {'C': 0.01}\n",
      "Parameter  {'C': 0.001} Train accuracy 0.808 Test accuracy 0.807 Train AUC 0.716 Test AUC 0.714  {'C': 0.001}\n",
      "Parameter  {'C': 0.0001} Train accuracy 0.808 Test accuracy 0.808 Train AUC 0.699 Test AUC 0.698  {'C': 0.0001}\n",
      "Parameter  {'C': 1e-05} Train accuracy 0.794 Test accuracy 0.794 Train AUC 0.691 Test AUC 0.691  {'C': 1e-05}\n",
      "Parameter  {'C': 1e-06} Train accuracy 0.789 Test accuracy 0.788 Train AUC 0.690 Test AUC 0.690  {'C': 1e-06}\n",
      "Parameter  {'C': 1e-07} Train accuracy 0.789 Test accuracy 0.788 Train AUC 0.690 Test AUC 0.690  {'C': 1e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_roc_auc'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter: ',gridSearch.best_params_, '\\n')\n",
    "results = gridSearch.cv_results_\n",
    "for trainAccuracy, testAccuracy, trainAUC, testAUC, parameter in zip(results['mean_train_accuracy'], \n",
    "                                                                     results['mean_test_accuracy'],\n",
    "                                                                     results['mean_train_roc_auc'],\n",
    "                                                                     results['mean_test_roc_auc'],\n",
    "                                                                     results['params']):\n",
    "    print('Parameter ', parameter, 'Train accuracy %.3f Test accuracy %.3f Train AUC %.3f Test AUC %.3f '\n",
    "          %(trainAccuracy, testAccuracy, trainAUC, testAUC), parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in terms of accuracy it does not matter much what the regularization parameter value is. The optimal parameter, among the chosen parameter values, is one, but the difference in test score between the eight first regularization parameter values is practically non-existent.\n",
    "\n",
    "### Logistic regression: Fitting and testing the model with the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = gridSearch.best_params_['C']\n",
    "logRegFinal = LogisticRegression(C=C, random_state=1)\n",
    "logRegFinal.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[11401   296]\n",
      " [ 2542   761]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.8108\n",
      "\n",
      "Train AUC: \n",
      " 0.6025454876099876\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[11375   292]\n",
      " [ 2561   772]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8098\n",
      "\n",
      "Test AUC: \n",
      " 0.6032976529846351\n"
     ]
    }
   ],
   "source": [
    "print('\\n###################  Training  ###############')\n",
    "yPredTrain = logRegFinal.predict(XTrain)#logReg.predict(XTrain)\n",
    "yPredTrain = (yPredTrain > 0.5)\n",
    "cm = confusion_matrix(\n",
    "    yTrain, yPredTrain) #.argmax(axis=1) Y_train_onehot.argmax(axis=1)\n",
    "print('\\nTraining Confusion matrix: \\n', cm)\n",
    "accScore = accuracy_score(yTrain, yPredTrain)\n",
    "print('\\nTraining Accuracy score: \\n', accScore)\n",
    "print('\\nTrain AUC: \\n', roc_auc_score(yTrain, yPredTrain))\n",
    "\n",
    "\n",
    "print('\\n###################  Testing  ###############')\n",
    "yPred = logRegFinal.predict(XTest)#logReg.predict(XTest)\n",
    "yPred = (yPred > 0.5)\n",
    "cm = confusion_matrix(\n",
    "    yTest, yPred) \n",
    "print('\\nTest Confusion matrix: \\n', cm)\n",
    "accScore = accuracy_score(yTest, yPred)\n",
    "print('\\nTest Accuracy score: \\n', accScore)\n",
    "print('\\nTest AUC: \\n', roc_auc_score(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accurcies and AUC's on the testing set is higher the cooresponding best mean numbers on the validation sets from the K-fold cross validation. We see that only about $1/3$ of the defaults get predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras NN\n",
    "We will now perform classification by deep neural networks. Keras is used. \n",
    "\n",
    "\n",
    "### Grid search\n",
    "We will apply Sci-Kit learn's grid search function in order to determine the optimal combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.815400 using {'hiddenLayers': 1, 'neurons': 200}\n",
      "0.813133 (0.002451) with: {'hiddenLayers': 1, 'neurons': 20}\n",
      "0.812067 (0.003594) with: {'hiddenLayers': 1, 'neurons': 100}\n",
      "0.815400 (0.002993) with: {'hiddenLayers': 1, 'neurons': 200}\n",
      "0.814533 (0.003021) with: {'hiddenLayers': 1, 'neurons': 500}\n",
      "0.801067 (0.006543) with: {'hiddenLayers': 3, 'neurons': 20}\n",
      "0.810067 (0.006614) with: {'hiddenLayers': 3, 'neurons': 100}\n",
      "0.810533 (0.004620) with: {'hiddenLayers': 3, 'neurons': 200}\n",
      "0.813733 (0.004878) with: {'hiddenLayers': 3, 'neurons': 500}\n",
      "0.784667 (0.004823) with: {'hiddenLayers': 5, 'neurons': 20}\n",
      "0.802067 (0.004521) with: {'hiddenLayers': 5, 'neurons': 100}\n",
      "0.812267 (0.004485) with: {'hiddenLayers': 5, 'neurons': 200}\n",
      "0.811333 (0.003920) with: {'hiddenLayers': 5, 'neurons': 500}\n",
      "0.813067 (0.003348) with: {'batch_size': 5}\n",
      "0.814400 (0.003374) with: {'batch_size': 10}\n",
      "0.797800 (0.010999) with: {'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "def createModel(neurons =50, hiddenLayers = 2):\n",
    "    model = tf.keras.Sequential()\n",
    "    neuronsPerLayer = neurons // (hiddenLayers + 1)\n",
    "    model.add(tf.keras.layers.Dense(neuronsPerLayer, activation='relu', input_dim=XTrain.shape[1]))\n",
    "    for i in range(hiddenLayers):\n",
    "        model.add(tf.keras.layers.Dense(neuronsPerLayer, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(Y_train_onehot.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=createModel, verbose=0)\n",
    "\n",
    "neurons = [20, 100, 200, 500]#, 50, 75]\n",
    "batch_size = [5, 10, 100]##, 40, 60, 80, 100]\n",
    "hiddenLayers = [1,3, 5]\n",
    "parameterGrid = [{'neurons': neurons, 'hiddenLayers': hiddenLayers},\n",
    "                 {'batch_size': batch_size}]\n",
    "folds = 3\n",
    "\n",
    "grid = GridSearchCV(estimator=model, cv=folds, param_grid=parameterGrid, n_jobs=-1)\n",
    "\n",
    "epochs = 10\n",
    "grid_result = grid.fit(XTrain, Y_train_onehot, epochs=epochs)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best combination of the chosen number of hidden layers and neuron numbers is one hidden layer and two hundred neurons. \n",
    "\n",
    "A batch size of 10 is the best among the chosen batch sizes.\n",
    "\n",
    "Next we apply the optimal combination of batch size and neuron number from the crossvalidation train a model on the full training set. The model based on the full training set will then be applied to measure the accuracy on predictions on the test set.  \n",
    "\n",
    "### FItting the best model: early stopping\n",
    "We see from the above that the validation accuracy declines for the highest number of neurons. The decline in validation accuracy is a sign of overfitting. In order to avoid overfitting we use a methods for \"early stopping\". Early stopping stops the simulations when the validation set performance has dropped a user given number of times in a row. In order for the model to be able to escape local minima, we allo the validation accuracy to drop a few times before breaking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs before early stopping:  4\n"
     ]
    }
   ],
   "source": [
    "hiddenLayers, neurons =  grid_result.best_params_['hiddenLayers'], grid_result.best_params_['neurons']\n",
    "batch_size = 10\n",
    "\n",
    "model = KerasClassifier(build_fn=createModel, verbose=0, neurons =neurons, hiddenLayers = hiddenLayers)\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                             min_delta=0,\n",
    "                                             patience=2, # argument represents the number of epochs before stopping once your loss starts to increase (stops improving)\n",
    "                                             verbose=0, \n",
    "                                             mode='auto')]#,\n",
    "                                             #restore_best_weights=True)] # Use best model\n",
    "history = model.fit(XTrain,\n",
    "                        Y_train_onehot,\n",
    "                        epochs=15, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=[XTest, Y_test_onehot],\n",
    "                        callbacks = callbacks)\n",
    "\n",
    "print('Number of epochs before early stopping: ', len(history.history['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices, accuracy scores and AUC-numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################  Training  ###############\n",
      "\n",
      "Training Confusion matrix: \n",
      " [[11172   525]\n",
      " [ 2190  1113]]\n",
      "\n",
      "Training Accuracy score: \n",
      " 0.819\n",
      "\n",
      "Train AUC: \n",
      " 0.6460415453879857\n",
      "\n",
      "###################  Testing  ###############\n",
      "\n",
      "Test Confusion matrix: \n",
      " [[11129   538]\n",
      " [ 2206  1127]]\n",
      "\n",
      "Test Accuracy score: \n",
      " 0.8170666666666667\n",
      "\n",
      "Test AUC: \n",
      " 0.6460104225902148\n"
     ]
    }
   ],
   "source": [
    "print('\\n###################  Training  ###############')\n",
    "yPredTrain = model.predict(XTrain)\n",
    "yPredTrain = (yPredTrain > 0.5)\n",
    "cm = confusion_matrix(\n",
    "    yTrain, yPredTrain) \n",
    "print('\\nTraining Confusion matrix: \\n', cm)\n",
    "accScore = accuracy_score(yTrain, yPredTrain)\n",
    "print('\\nTraining Accuracy score: \\n', accScore)\n",
    "print('\\nTrain AUC: \\n', roc_auc_score(yTrain, yPredTrain))\n",
    "\n",
    "\n",
    "print('\\n###################  Testing  ###############')\n",
    "yPred = model.predict(XTest)\n",
    "yPred = (yPred > 0.5)\n",
    "cm = confusion_matrix(\n",
    "    yTest, yPred) \n",
    "print('\\nTest Confusion matrix: \\n', cm)\n",
    "accScore = accuracy_score(yTest, yPred)\n",
    "print('\\nTest Accuracy score: \\n', accScore)\n",
    "print('\\nTest AUC: \\n', roc_auc_score(yTest, yPred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about a third of the defaulting customers are correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok accuracy. Not impressing performance when it compes to predicting the defaults. Only a 3rd of the defaults in the test set is correctly predicted. <br>\n",
    "\n",
    "Yeh and Lien (2009) get: <br>\n",
    "Training accuracy: 0.81<br>\n",
    "Training AUC: 0.55 <br>\n",
    "Testing accuracy: 0.83<br>\n",
    "Testing AUC: 0.54 <br>\n",
    "\n",
    "We see that we get the accuracy the the same level as Yeh and Lien (2009), but the AUC is better. However, it is unclear wheteher it acutally is AUC Yeh and Lien (2009) applies, as they call it area ratio.<br>\n",
    "\n",
    "Pyzhov and Pyzhov (2017) get about the same acuracy as we do, but higher AUCs.\n",
    "\n",
    "# Principal component analyses\n",
    "We will explore the effects of training the network with principcal components. For many of the networks we do not expect the introduction of principcal components to improve the performance considerably. The difference between training and testing accuracy is small for many of the networks, indicating that there is little overfitting. Maybe for the network setuos where there are larger deviations between training and testing accuracy, the networks with mutiple hidden layers, there can be gains from introducing principcal components. \n",
    "\n",
    "We use the principal components that explain 95 per cent of total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84870179 0.04871288 0.02708606 0.01606241 0.0144179 ]\n",
      "0.9549810379752075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "'''\n",
    "pca = PCA(n_components = 4)\n",
    "pca.fit_transform(X)\n",
    "#pca.components_.T[:,0] # Displays component\n",
    "pca.explained_variance_ratio_\n",
    "'''\n",
    "\n",
    "pca = PCA(n_components = 0.95)\n",
    "Xreduced = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 50)                300       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 10,602\n",
      "Trainable params: 10,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "15000/15000 [==============================] - 2s 135us/step - loss: 5.1919 - acc: 0.6777 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 2/100\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 3/100\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 4/100\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 5/100\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n",
      "Epoch 6/100\n",
      "15000/15000 [==============================] - 1s 80us/step - loss: 3.5417 - acc: 0.7803 - val_loss: 3.5890 - val_acc: 0.7773\n"
     ]
    }
   ],
   "source": [
    "trainingShare = 0.5 \n",
    "XTrain, XTest, yTrain, yTest=train_test_split(Xreduced, y, train_size=trainingShare, \\\n",
    "                                              test_size = 1-trainingShare)\n",
    "Y_train_onehot, Y_test_onehot = to_categorical(yTrain), to_categorical(yTest)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu', input_dim=XTrain.shape[1]))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(Y_train_onehot.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             min_delta = 0,\n",
    "                                             patience=5,\n",
    "                                             verbose=0,\n",
    "                                             mode='auto')]\n",
    "\n",
    "\n",
    "history = model.fit(XTrain,\n",
    "                    Y_train_onehot, \n",
    "                    epochs=100, \n",
    "                    batch_size=30,\n",
    "                    validation_data=[XTest, Y_test_onehot],\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both training and testing accuracy is reduced when using Principal components as predictors instead of all of the original features. However, we also observe that the difference between training and validation accuracy is smaller when using principal components as predictors instead of all the features from the original data set.\n",
    "\n",
    "## Kernel PCA\n",
    "We will try Sci-Kit learn's kernel PCA method. Kernel PCA, kPca, is a PCA-method that allows for non-linearity. <mark> More about this!\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-64be72b0cefc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkPCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKernelPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#kPCA = TruncatedSVD(n_components=5, algorithm='arpack')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkPCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \"\"\"\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambdas_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \"\"\"\n\u001b[1;32m    237\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py\u001b[0m in \u001b[0;36m_get_kernel\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    164\u001b[0m         return pairwise_kernels(X, Y, metric=self.kernel,\n\u001b[1;32m    165\u001b[0m                                 \u001b[0mfilter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                 **params)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_kernels\u001b[0;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown kernel %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[0;34m(X, Y, gamma)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# exponentiate K in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA, TruncatedSVD\n",
    "kPCA = KernelPCA(n_components=2, kernel='rbf', gamma=10) #0.04\n",
    "#kPCA = TruncatedSVD(n_components=5, algorithm='arpack')\n",
    "X_reduced = kPCA.fit_transform(X)\n",
    "print(kPCA.explained_variance_ratio_)\n",
    "print(np.sum(kPCA.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kPCA did not work. It gave memory error.\n",
    "\n",
    "# Support Vector Machines (SVM)\n",
    "We will now apply the SVM classifier to make the classification. We start by running the standard SVM estimator, and then we try alternative methods that potentially increase accuracy in the presence of non-linearity in the data. By \"non-linearity\" we mean that the labels cannot be separated by a linear classification plane (line in 2D, 2D plane in 3D, hyperplane for higher dimensions than 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Yeh, I-C. and Lien, C-h. (2009). The comparisons of data mining techniques for the predictive\n",
    "accuracy of probability of default of credit card clients. <br>\n",
    "_Expert Systems with Applications_ 36 (2009) 2473–2480. <br>\n",
    "https://bradzzz.gitbooks.io/ga-seattle-dsi/content/dsi/dsi_05_classification_databases/2.1-lesson/assets/datasets/DefaultCreditCardClients_yeh_2009.pdf\n",
    "\n",
    "Pyzhov, V. and Pyzhov, S. (2017). Comparison of methods of data mining\n",
    "techniques for the predictive accuracy. _MPRA Paper_ No. 79326. <br>\n",
    "https://mpra.ub.uni-muenchen.de/79326/1/MPRA_paper_79326.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
